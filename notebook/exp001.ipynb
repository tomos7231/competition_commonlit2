{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4d61fc-85c1-41a5-a90a-8f7d3cef5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 19 13:39:42 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   46C    P8    16W / 300W |  10099MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef23f706-f84a-4274-a21f-62d3ceb0b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    AUTHOR = \"wanwan7123\"\n",
    "\n",
    "    NAME = \"commonlit-exp001-deberta-v3-base\"\n",
    "    MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "    ROOT = \"/notebooks\"\n",
    "\n",
    "    apex=True\n",
    "    seed = 42\n",
    "    num_fold = 4\n",
    "    trn_fold = [0, 1, 2, 3]\n",
    "    batch_size = 16\n",
    "    n_epochs = 3\n",
    "    max_len = 512\n",
    "    target_list = [\"content\", \"wording\"]\n",
    "    \n",
    "    weight_decay = 0.01\n",
    "    scheduler='cosine'\n",
    "    betas = (0.9, 0.999)\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 2e-5\n",
    "    lr_weight_decay = 0.95\n",
    "    min_lr = 1e-6\n",
    "    eps = 1e-6\n",
    "    eval_step = 100\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps_rate=0.1\n",
    "    clip_grad_norm = 1000\n",
    "    gradient_accumulation_steps = 1\n",
    "    \n",
    "    # GPU Optimize Settings\n",
    "    gpu_optimize_config= {\n",
    "        \"freezing\": False,\n",
    "        \"gradient_checkpoint\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e005db68-c9ac-4d8f-9cf0-0ad993be0bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.10.1+cu113 in /usr/local/lib/python3.9/dist-packages (1.10.1+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: text-unidecode in /usr/local/lib/python3.9/dist-packages (1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import joblib\n",
    "import random\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from ast import literal_eval\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    KFold, \n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "! pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!pip install text-unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab3cbd-a73c-4b3f-aee6-cf7b3bb3e257",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a32846-749b-4981-bd31-94c4beeb650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(cfg):\n",
    "    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # pip install\n",
    "    ! pip install -qq transformers==4.16.2\n",
    "    ! pip install -qq tokenizers==0.11.6\n",
    "    ! pip install -qq transformers[sentencepiece]\n",
    "\n",
    "    # set dirs    \n",
    "    cfg.INPUT = Path(f\"{cfg.ROOT}/input\")\n",
    "    cfg.OUTPUT = Path(f\"{cfg.ROOT}/output\")\n",
    "    cfg.EXP = cfg.OUTPUT / cfg.NAME\n",
    "\n",
    "    cfg.api_path = f\"{cfg.ROOT}/kaggle.json\"\n",
    "\n",
    "    cfg.EXP_MODEL = cfg.EXP / \"model\"\n",
    "    cfg.EXP_FIG = cfg.EXP / \"fig\"\n",
    "    cfg.EXP_PREDS = cfg.EXP / \"preds\"\n",
    "\n",
    "    # make dirs\n",
    "    for d in [cfg.EXP, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n",
    "        d.mkdir(exist_ok=True)\n",
    "        \n",
    "    # use kaggle api (need kaggle token)\n",
    "    f = open(cfg.api_path, 'r')\n",
    "    json_data = json.load(f) \n",
    "    os.environ['KAGGLE_USERNAME'] = json_data['username']\n",
    "    os.environ['KAGGLE_KEY'] = json_data['key']\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "def dataset_create_new(dataset_name, upload_dir):\n",
    "    dataset_metadata = {}\n",
    "    dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n",
    "    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n",
    "    dataset_metadata['title'] = dataset_name\n",
    "    with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=4)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b9b321-c43a-4428-99d5-430ca1917b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Utils\n",
    "# =====================\n",
    "# Seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# KFold\n",
    "def get_kfold(train, n_splits, seed):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train)\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_stratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupkfold(train, target_col, group_col, n_splits):\n",
    "    kf = GroupKFold(n_splits=n_splits)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_groupstratifiedkfold(train, target_col, group_col, n_splits, seed):\n",
    "    kf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col], train[group_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_multilabelstratifiedkfold(train, target_col, n_splits, seed):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    generator = kf.split(train, train[target_col])\n",
    "    fold_series = []\n",
    "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
    "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
    "    fold_series = pd.concat(fold_series).sort_index()\n",
    "    return fold_series\n",
    "\n",
    "def get_commonlit_fold(train):\n",
    "    id2fold = {\n",
    "        \"814d6b\": 0,\n",
    "        \"39c16e\": 1,\n",
    "        \"3b9047\": 2,\n",
    "        \"ebad26\": 3,\n",
    "    }\n",
    "    train[\"fold\"] = train[\"prompt_id\"].map(id2fold)\n",
    "    return train[\"fold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0215b90-d40f-4e2b-bccb-c516afb12c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcrmse(cfg, preds, df):\n",
    "    all_score = 0\n",
    "    for i, column in enumerate(cfg.target_list):\n",
    "        score = np.sqrt(mean_squared_error(preds[:, i], df[column]))\n",
    "        all_score += score/len(cfg.target_list)\n",
    "    return all_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6334798-8f53-419e-a378-63d8daeda473",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b7b941-e196-4349-8aac-c3e3ca796033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_unidecode import unidecode\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e4c1c0-9232-4d4b-87ff-bec7d91653f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_features(df):\n",
    "    df['text'] = df['text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49940cc-886e-4f5f-a17e-14ad2a007857",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a44555a-ba86-4e10-9295-4bc5a5b8c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.text = df['text'].to_numpy()\n",
    "        self.labels = df[cfg.target_list].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.prepare_input(self.cfg, self.text[index])\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        return inputs, label\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_input(cfg, text):\n",
    "        inputs = cfg.tokenizer(text,\n",
    "                               add_special_tokens=True,\n",
    "                               max_length=cfg.max_len,\n",
    "                               padding=\"max_length\",\n",
    "                               truncation=True,\n",
    "                               return_offsets_mapping=False)\n",
    "        inputs['input_ids'] = torch.tensor(\n",
    "            inputs['input_ids'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs['attention_mask'] = torch.tensor(\n",
    "            inputs['attention_mask'],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a4ac4-3afd-4793-944a-2f30ad185dc7",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d234a9-f3e5-4913-a040-1d5e76f90153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "        nn.Linear(in_dim, in_dim),\n",
    "        nn.LayerNorm(in_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(in_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        w = self.attention(last_hidden_state).float()\n",
    "        w[attention_mask==0]=float('-inf')\n",
    "        w = torch.softmax(w,1)\n",
    "        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n",
    "        return attention_embeddings\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.gpu_optimize_config = cfg.gpu_optimize_config\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout\": 0.,\n",
    "                \"hidden_dropout_prob\": 0.,\n",
    "                \"attention_dropout\": 0.,\n",
    "                \"attention_probs_dropout_prob\": 0,\n",
    "            }\n",
    "        )\n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            cfg.MODEL_PATH,\n",
    "            config=self.config\n",
    "        )\n",
    "\n",
    "        self.pool = AttentionPooling(self.config.hidden_size)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 2)\n",
    "        self._init_weights(self.fc)\n",
    "        self.ln = nn.LayerNorm(self.config.hidden_size)\n",
    "        self._init_weights(self.ln)\n",
    "\n",
    "        # Freeze\n",
    "        if self.gpu_optimize_config['freezing']:\n",
    "            freeze(self.backbone.encoder.layer[:8])\n",
    "\n",
    "        # Gradient Checkpointing\n",
    "        if self.gpu_optimize_config['gradient_checkpoint']:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.backbone(**inputs)\n",
    "        last_state = outputs[0]\n",
    "        feature = self.pool(last_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        # batch, hidden_size\n",
    "        feature = self.feature(inputs)\n",
    "        # batch, 2\n",
    "        output = self.fc(self.ln(feature))\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.SmoothL1Loss(reduction='mean')\n",
    "            loss = loss_fct(output, labels)\n",
    "            return loss, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9216ad9a-d7f4-40bd-af0d-4ff0ff7499aa",
   "metadata": {},
   "source": [
    "# optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975ed9d1-27e5-49d9-9f6b-cf4de08469cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(cfg, model):\n",
    "    model_type = 'backbone'\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n",
    "             'lr': cfg.decoder_lr, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n",
    "    layers.reverse()\n",
    "    lr = cfg.encoder_lr\n",
    "    for layer in layers:\n",
    "        lr *= cfg.lr_weight_decay\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": cfg.weight_decay,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "                \"lr\": lr,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b424bc0d-b830-4a5d-8982-2b74b8e4ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=int(num_train_steps*cfg.num_warmup_steps_rate), num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7ed151-7342-4156-b1ec-22f3a9078f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinit_bert(model):\n",
    "    for layer in model.backbone.encoder.layer[-1:]:\n",
    "        for module in layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62efab1e-1350-4c34-adeb-972b649d40ba",
   "metadata": {},
   "source": [
    "# fgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d86019-c59e-4f2f-835e-487d4eb05ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=0.3, emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870dffaa-01c5-40d5-a307-ce6d49478ebe",
   "metadata": {},
   "source": [
    "# eval,train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dcde54a-4e69-4dd0-bd89-13585a68945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(cfg, valid_loader, model, valid_df, fold, best_val_preds, best_val_score):\n",
    "    val_preds = []\n",
    "    val_losses = []\n",
    "    val_nums = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valid_loader, total=len(valid_loader)) as pbar:\n",
    "            for (inputs, labels) in pbar:\n",
    "                inputs = collate(inputs)\n",
    "                for k, v in inputs.items():\n",
    "                    inputs[k] = v.to(cfg.device)\n",
    "                labels = labels.to(cfg.device)\n",
    "                with autocast():\n",
    "                    loss, output = model(inputs, labels)\n",
    "                \n",
    "                output = output.detach().cpu().numpy()\n",
    "                val_preds.append(output)\n",
    "                val_losses.append(loss.item() * len(labels))\n",
    "                val_nums.append(len(labels))\n",
    "                pbar.set_postfix({\n",
    "                    'val_loss': loss.item()\n",
    "                })\n",
    "\n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_loss = sum(val_losses) / sum(val_nums)\n",
    "    score = mcrmse(cfg, val_preds, valid_df)\n",
    "\n",
    "    val_log = {\n",
    "        'val_loss': val_loss,\n",
    "        'mcrmse': score\n",
    "    }\n",
    "    display(val_log)\n",
    "\n",
    "    if best_val_score > score:\n",
    "        print('\\033[31m'+'save model weight'+'\\033[0m')\n",
    "        best_val_preds = val_preds\n",
    "        best_val_score = score\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            cfg.EXP_MODEL / f\"fold{fold}.pth\"\n",
    "        )\n",
    "    \n",
    "    return best_val_preds, best_val_score\n",
    "\n",
    "def training(cfg, train):\n",
    "    # =====================\n",
    "    # Training\n",
    "    # =====================\n",
    "    set_seed(cfg.seed)\n",
    "    oof_pred = np.zeros((len(train), 2), dtype=np.float32)\n",
    "    fold_score = []\n",
    "\n",
    "    for fold in cfg.trn_fold:\n",
    "        # dataset, dataloader\n",
    "        train_df = train.loc[cfg.folds!=fold]\n",
    "        valid_df = train.loc[cfg.folds==fold]\n",
    "        train_idx = list(train_df.index)\n",
    "        valid_idx = list(valid_df.index)\n",
    "\n",
    "        # Datasetの設定\n",
    "        train_dataset = TrainDataset(cfg, train_df)\n",
    "        valid_dataset = TrainDataset(cfg, valid_df)\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        # model\n",
    "        model = CustomModel(cfg)\n",
    "        torch.save(model.config, cfg.EXP_MODEL / 'config.pth')\n",
    "        # model = reinit_bert(model)\n",
    "        model = model.to(cfg.device)\n",
    "\n",
    "        # optimizer, scheduler\n",
    "        optimizer_grouped_parameters = get_optimizer_grouped_parameters(cfg, model)\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas, weight_decay=cfg.weight_decay)\n",
    "\n",
    "        num_train_steps = int(len(train_df) / cfg.batch_size * cfg.n_epochs)\n",
    "        scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n",
    "\n",
    "        # enable FGM\n",
    "        # fgm = FGM(model)\n",
    "\n",
    "        # model-training\n",
    "        best_val_preds = None\n",
    "        best_val_score = 9999\n",
    "        \n",
    "        for epoch in range(cfg.n_epochs):\n",
    "            # training\n",
    "            print(f\"# ============ start epoch:{epoch} ============== #\")\n",
    "            train_losses = []\n",
    "            train_nums = []\n",
    "            model.train() \n",
    "            scaler = GradScaler(enabled=cfg.apex)\n",
    "            with tqdm(train_loader, total=len(train_loader)) as pbar:\n",
    "                for step, (inputs, labels) in enumerate(pbar):\n",
    "                    inputs = collate(inputs)\n",
    "                    for k, v in inputs.items():\n",
    "                        inputs[k] = v.to(cfg.device)\n",
    "                    labels = labels.to(cfg.device)\n",
    "                    with autocast(enabled=cfg.apex):\n",
    "                        loss, output = model(inputs, labels)\n",
    "\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': loss.item(),\n",
    "                        'lr': scheduler.get_lr()[0]\n",
    "                    })\n",
    "                    train_losses.append(loss.item() * len(labels))\n",
    "                    train_nums.append(len(labels))\n",
    "\n",
    "                    if cfg.gradient_accumulation_steps > 1:\n",
    "                        loss = loss / cfg.gradient_accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    # FGM attack\n",
    "                    # fgm.attack()\n",
    "                    # with autocast(enabled=cfg.apex):\n",
    "                    #     loss_adv, _ = model(inputs, labels)\n",
    "                    # scaler.scale(loss_adv).backward()\n",
    "                    # fgm.restore()\n",
    "                    \n",
    "                    if cfg.clip_grad_norm is not None:\n",
    "                        # scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(),\n",
    "                            cfg.clip_grad_norm\n",
    "                        )\n",
    "                        \n",
    "                    if (step+1) % cfg.gradient_accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                        scheduler.step()\n",
    "\n",
    "                    if step % cfg.eval_step == 0 and step != 0:\n",
    "                        print(f'fold: {fold}, epoch: {epoch}, step: {step}')\n",
    "                        best_val_preds, best_val_score = evaluating(\n",
    "                            cfg, valid_loader,\n",
    "                            model,\n",
    "                            valid_df,\n",
    "                            fold,\n",
    "                            best_val_preds,\n",
    "                            best_val_score,\n",
    "                        )\n",
    "                        model.train()\n",
    "\n",
    "            train_loss = sum(train_losses)/sum(train_nums)\n",
    "            train_log = {\n",
    "                'train_loss':train_loss\n",
    "            }\n",
    "            display(train_log)\n",
    "\n",
    "            # evaluating(epoch)\n",
    "            print(f'fold: {fold}, epoch: {epoch}, complete')\n",
    "            best_val_preds, best_val_score = evaluating(\n",
    "                cfg, valid_loader,\n",
    "                model,\n",
    "                valid_df,\n",
    "                fold,\n",
    "                best_val_preds,\n",
    "                best_val_score,\n",
    "            )\n",
    "\n",
    "        oof_pred[valid_idx] = best_val_preds.astype(np.float32)\n",
    "        np.save(cfg.EXP_PREDS / f'oof_pred_fold{fold}.npy', best_val_preds)\n",
    "        fold_score.append(best_val_score)\n",
    "        del model; gc.collect()\n",
    "\n",
    "    np.save(cfg.EXP_PREDS / 'oof_pred.npy', oof_pred)\n",
    "\n",
    "    # =====================\n",
    "    # scoring\n",
    "    # =====================\n",
    "    score = mcrmse(cfg, oof_pred, train)\n",
    "    print('fold score：', fold_score)\n",
    "    print('CV:', round(score, 4))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba9659-8a32-4eae-abe5-3bb09fbd1fb4",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393a583a-01fa-47fe-ad15-d26a818e6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n",
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:0 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a168f1309f4f2e933fdb605e1d47d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 0, step: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da3a1b97ad1444f996b382b9f2ed072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.29880968717045925, 'mcrmse': 0.815633363648757}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 0, epoch: 0, step: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b04be6d3d9d47169a7acbec690db78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.215029361917015, 'mcrmse': 0.6802364660222491}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msave model weight\u001b[0m\n",
      "fold: 0, epoch: 0, step: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c324f788cbe743eeb8a0019b2b4330c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.24235529739642728, 'mcrmse': 0.7196601657510585}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.19370497219146243}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 0, complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5080b99ab742b89311c57c9f4266f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.2591083083548334, 'mcrmse': 0.7456954479893014}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:1 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644e226c0f1f4b5c87a64446ebce90cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 1, step: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b6cb8fdbeb45d883ca7b79484801f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.21642341753189281, 'mcrmse': 0.6854928253851489}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 1, step: 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ab665082b4454783ea36cbd4e298c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.24989773869406387, 'mcrmse': 0.7374834912361177}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 1, step: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1dcfb4dbb14ffeb8986ba00754a489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.2554541094834872, 'mcrmse': 0.744997135626822}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.09692327165729785}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 1, complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b823d00d885d4fef9f8eced0e5a6f387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.2479986091256682, 'mcrmse': 0.7324969693869715}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ============ start epoch:2 ============== #\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88588c12bfd34d68810f2d356e022798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0, epoch: 2, step: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1bf52989a54aacbb895d63d846a767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m cfg\u001b[38;5;241m.\u001b[39mfolds \u001b[38;5;241m=\u001b[39m get_commonlit_fold(train)\n\u001b[1;32m     22\u001b[0m cfg\u001b[38;5;241m.\u001b[39mfolds\u001b[38;5;241m.\u001b[39mto_csv(cfg\u001b[38;5;241m.\u001b[39mEXP_PREDS \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolds.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [15], line 148\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(cfg, train)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39meval_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m             best_val_preds, best_val_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluating\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbest_val_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbest_val_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m             model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    158\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(train_losses)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(train_nums)\n",
      "Cell \u001b[0;32mIn [15], line 8\u001b[0m, in \u001b[0;36mevaluating\u001b[0;34m(cfg, valid_loader, model, valid_df, fold, best_val_preds, best_val_score)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(valid_loader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(valid_loader)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m----> 8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      9\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m collate(inputs)\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [9], line 11\u001b[0m, in \u001b[0;36mTrainDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 11\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs, label\n",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m, in \u001b[0;36mTrainDataset.prepare_input\u001b[0;34m(cfg, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_input\u001b[39m(cfg, text):\n\u001b[0;32m---> 17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                           \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     24\u001b[0m         inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     25\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     28\u001b[0m         inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     29\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[1;32m     30\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2463\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2444\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2445\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2460\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2461\u001b[0m     )\n\u001b[1;32m   2462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2481\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2536\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2527\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2528\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2529\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2533\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2534\u001b[0m )\n\u001b[0;32m-> 2536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:647\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    651\u001b[0m     first_ids,\n\u001b[1;32m    652\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    667\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:164\u001b[0m, in \u001b[0;36mDebertaV2Tokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n\u001b[1;32m    163\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:335\u001b[0m, in \u001b[0;36mSPMTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 335\u001b[0m pieces \u001b[38;5;241m=\u001b[39m [_norm(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pieces]\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pieces\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2.py:335\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 335\u001b[0m pieces \u001b[38;5;241m=\u001b[39m [_norm(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pieces]\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pieces\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setup\n",
    "cfg = setup(Config)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import tokenizers\n",
    "import sentencepiece\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "\n",
    "# main\n",
    "train = pd.read_csv(cfg.INPUT / \"summaries_train.csv\")\n",
    "prompts_train = pd.read_csv(cfg.INPUT / \"prompts_train.csv\")\n",
    "\n",
    "train = processing_features(train)\n",
    "\n",
    "cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_PATH)\n",
    "cfg.tokenizer.save_pretrained(cfg.EXP / 'tokenizer')\n",
    "cfg.folds = get_commonlit_fold(train)\n",
    "cfg.folds.to_csv(cfg.EXP_PREDS / 'folds.csv')\n",
    "score = training(cfg, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c069c-81db-4d5e-b4d6-36a7213ee92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d092b7-3eb3-4eaf-96a3-98bca99cae03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
